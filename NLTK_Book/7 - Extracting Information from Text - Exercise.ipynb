{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist # need this to access \"most_common\" method\n",
    "\n",
    "import urllib2\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from __future__  import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a tag pattern to match noun phrases containing plural head nouns, e.g. \"many/JJ researchers/NNS\", \"two/CD weeks/NNS\", \"both/DT new/JJ positions/NNS\". Try to do this by generalizing the tag pattern that handled singular noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern = 'NNS: {<JJ|CD|DT>+<NNS>+}'\n",
    "cp = nltk.RegexpParser(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Three',\n",
       " 'men',\n",
       " 'are',\n",
       " 'caught',\n",
       " 'drunk',\n",
       " 'driving',\n",
       " '.',\n",
       " 'Two',\n",
       " 'of',\n",
       " 'them',\n",
       " 'have',\n",
       " 'four',\n",
       " 'beers',\n",
       " '.']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '''\n",
    "Three men are caught drunk driving. Two of them have four beers.\n",
    "\n",
    "'''\n",
    "words = word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Three', 'CD'),\n",
       " ('men', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('caught', 'VBN'),\n",
       " ('drunk', 'JJ'),\n",
       " ('driving', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Two', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('them', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('four', 'CD'),\n",
       " ('beers', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tags = nltk.pos_tag(words)\n",
    "word_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NNS Three/CD men/NNS)\n",
      "  are/VBP\n",
      "  caught/VBN\n",
      "  drunk/JJ\n",
      "  driving/NN\n",
      "  ./.\n",
      "  Two/CD\n",
      "  of/IN\n",
      "  them/PRP\n",
      "  have/VBP\n",
      "  (NNS four/CD beers/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "result = cp.parse(word_tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "An early definition of chunk was the material that occurs between chinks. Develop a chunker that starts by putting the whole sentence in a single chunk, and then does the rest of its work solely by chinking. Determine which tags (or tag sequences) are most likely to make up chinks with the help of your own utility program. Compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = '''\n",
    "NP: \n",
    "   {<.*>+}    # chunk everything\n",
    "   }<DT|RP>+{ # chink sequences of DT or RP\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use one of brown corpus' sentence to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The',\n",
       " u'Fulton',\n",
       " u'County',\n",
       " u'Grand',\n",
       " u'Jury',\n",
       " u'said',\n",
       " u'Friday',\n",
       " u'an',\n",
       " u'investigation',\n",
       " u'of',\n",
       " u\"Atlanta's\",\n",
       " u'recent',\n",
       " u'primary',\n",
       " u'election',\n",
       " u'produced',\n",
       " u'``',\n",
       " u'no',\n",
       " u'evidence',\n",
       " u\"''\",\n",
       " u'that',\n",
       " u'any',\n",
       " u'irregularities',\n",
       " u'took',\n",
       " u'place',\n",
       " u'.']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = nltk.corpus.brown.sents()\n",
    "sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'The', 'DT'),\n",
       " (u'Fulton', 'NNP'),\n",
       " (u'County', 'NNP'),\n",
       " (u'Grand', 'NNP'),\n",
       " (u'Jury', 'NNP'),\n",
       " (u'said', 'VBD'),\n",
       " (u'Friday', 'NNP'),\n",
       " (u'an', 'DT'),\n",
       " (u'investigation', 'NN'),\n",
       " (u'of', 'IN'),\n",
       " (u\"Atlanta's\", 'NNP'),\n",
       " (u'recent', 'JJ'),\n",
       " (u'primary', 'JJ'),\n",
       " (u'election', 'NN'),\n",
       " (u'produced', 'VBD'),\n",
       " (u'``', '``'),\n",
       " (u'no', 'DT'),\n",
       " (u'evidence', 'NN'),\n",
       " (u\"''\", \"''\"),\n",
       " (u'that', 'IN'),\n",
       " (u'any', 'DT'),\n",
       " (u'irregularities', 'NNS'),\n",
       " (u'took', 'VBD'),\n",
       " (u'place', 'NN'),\n",
       " (u'.', '.')]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tags = nltk.pos_tag(sent[0])\n",
    "word_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (NP Fulton/NNP County/NNP Grand/NNP Jury/NNP said/VBD Friday/NNP)\n",
      "  an/DT\n",
      "  (NP\n",
      "    investigation/NN\n",
      "    of/IN\n",
      "    Atlanta's/NNP\n",
      "    recent/JJ\n",
      "    primary/JJ\n",
      "    election/NN\n",
      "    produced/VBD\n",
      "    ``/``)\n",
      "  no/DT\n",
      "  (NP evidence/NN ''/'' that/IN)\n",
      "  any/DT\n",
      "  (NP irregularities/NNS took/VBD place/NN ./.))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(pattern)\n",
    "print (cp.parse(word_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"an/DT\", \"no/DT\", \"any/DT\" are chinked out using the pattern we assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a tag pattern to cover noun phrases that contain gerunds, e.g. \"the/DT receiving/VBG end/NN\", \"assistant/NN managing/VBG editor/NN\". Add these patterns to the grammar, one per line. Test your work using some tagged sentences of your own devising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gerund_patterns = '''\n",
    "GERUNDS: \n",
    "       {<DT>*<VB.?>+<DT>?<NNS>+}    # chunk everything\n",
    "       {<NN>*<VB.?>+<DT>?<NNS>+} # chink sequences of DT or RP\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  grand/JJ\n",
      "  jury/NN\n",
      "  commented/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  number/NN\n",
      "  of/IN\n",
      "  other/JJ\n",
      "  topics/NNS\n",
      "  ,/,\n",
      "  among/IN\n",
      "  them/PRP\n",
      "  the/DT\n",
      "  Atlanta/NNP\n",
      "  and/CC\n",
      "  Fulton/NNP\n",
      "  County/NNP\n",
      "  purchasing/NN\n",
      "  departments/NNS\n",
      "  which/WDT\n",
      "  it/PRP\n",
      "  said/VBD\n",
      "  ``/``\n",
      "  are/VBP\n",
      "  well/RB\n",
      "  operated/VBN\n",
      "  and/CC\n",
      "  follow/VB\n",
      "  generally/RB\n",
      "  (GERUNDS accepted/VBN practices/NNS)\n",
      "  which/WDT\n",
      "  inure/VBP\n",
      "  to/TO\n",
      "  the/DT\n",
      "  best/JJS\n",
      "  interest/NN\n",
      "  of/IN\n",
      "  both/DT\n",
      "  governments/NNS\n",
      "  ''/''\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sent = nltk.corpus.brown.sents()\n",
    "# word_tags = nltk.pos_tag(sent)\n",
    "word_tags = nltk.pos_tag(sent[6])\n",
    "cp = nltk.RegexpParser(gerund_patterns)\n",
    "print (cp.parse(word_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S Can/MD you/PRP (GERUNDS help/VB fixing/VBG the/DT issues/NNS) ?/.)\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = 'Can you help fixing the issues?'\n",
    "word_tags = nltk.pos_tag(word_tokenize(sentence_1))\n",
    "cp = nltk.RegexpParser(gerund_patterns)\n",
    "print (cp.parse(word_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write one or more tag patterns to handle coordinated noun phrases, e.g. \"July/NNP and/CC August/NNP\", \"all/DT your/PRP$ managers/NNS and/CC supervisors/NNS\", \"company/NN courts/NNS and/CC adjudicators/NNS\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the following patterns handle some coordinated noun phrases:\n",
    "\n",
    "cnp = '''\n",
    "SENARIO_1: {<NN.*>+<CC>*<NN.*>+}\n",
    "SENARIO_2: {<DT><PRP$><NNS><CC><NNS>}\n",
    "SENARIO_3: {<NN.*>*<CC>*<NN.*>+}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(cnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'July and August. All your managers and supervisors. Company courts and adjudicators.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (SENARIO_1 July/NNP and/CC August/NNP)\n",
      "  ./.\n",
      "  All/DT\n",
      "  your/PRP$\n",
      "  (SENARIO_1 managers/NNS and/CC supervisors/NNS)\n",
      "  ./.\n",
      "  (SENARIO_1 Company/NN courts/NNS and/CC adjudicators/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "word_tags = nltk.pos_tag(word_tokenize(sentence))\n",
    "print (cp.parse(word_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Carry out the following evaluation tasks for any of the chunkers you have developed earlier. (Note that most chunking corpora contain some internal inconsistencies, such that any reasonable rule-based approach will produce errors.)\n",
    "\n",
    "- Evaluate your chunker on 100 sentences from a chunked corpus, and report the precision, recall and F-measure.\n",
    "- Use the chunkscore.missed() and chunkscore.incorrect() methods to identify the errors made by your chunker. Discuss.\n",
    "- Compare the performance of your chunker to the baseline chunker discussed in the evaluation section of this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the chunked corpus here:\n",
    "\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.a"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Evaluate your chunker on 100 sentences from a chunked corpus, and report the precision, recall and F-measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  42.1%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "pattern = 'NNS: {<[NPJ].*>+}'\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print (cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try UnigramChunker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence): \n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print (unigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BigramChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence): \n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.4%%\n",
      "    Precision:     82.6%%\n",
      "    Recall:        87.5%%\n",
      "    F-Measure:     85.0%%\n"
     ]
    }
   ],
   "source": [
    "bigram_chunker = BigramChunker(test_sents)\n",
    "print (bigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TrigramChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.TrigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence): \n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.5%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.6%%\n"
     ]
    }
   ],
   "source": [
    "trigram_chunker = TrigramChunker(train_sents)\n",
    "print (trigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.b"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use the chunkscore.missed() and chunkscore.incorrect() methods to identify the errors made by your chunker. Discuss."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "chunkscore:\n",
    "\n",
    "http://www.nltk.org/api/nltk.chunk.html?highlight=chunkscore#nltk.chunk.util.ChunkScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ChunkScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunkscore = nltk.chunk.ChunkScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('F Measure:', 0.8458218909376227)\n",
      "('Chunkscore:', <ChunkScoring of 12422 chunks>)\n"
     ]
    }
   ],
   "source": [
    "for correct in test_sents:   \n",
    "    guess = trigram_chunker.parse(correct.leaves())   \n",
    "    chunkscore.score(correct, guess)   \n",
    "print('F Measure:', chunkscore.f_measure()) \n",
    "print('Chunkscore:', chunkscore) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ImmutableTree('NP', [(u'quite', u'RB'), (u'a', u'DT'), (u'bit', u'NN')]),\n",
       " ImmutableTree('NP', [(u'so-called', u'JJ'), (u'analog', u'NN'), (u'integrated', u'VBN'), (u'circuits', u'NNS')]),\n",
       " ImmutableTree('NP', [(u'certain', u'JJ'), (u'Santa', u'NNP'), (u'Monica', u'NNP'), (u'Mountain', u'NNP'), (u'trails', u'NNS')]),\n",
       " ImmutableTree('NP', [(u'next', u'JJ'), (u'May', u'NNP')]),\n",
       " ImmutableTree('NP', [(u'``', u'``'), (u'chinless', u'JJ'), (u\"''\", u\"''\"), (u'Dan', u'NNP'), (u'Shaughnessy', u'NNP')])]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkscore.missed()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ImmutableTree('NP', [(u'Business', u'NNP'), (u'Channel', u'NNP'), (u'cable', u'NN'), (u'network', u'NN')]),\n",
       " ImmutableTree('NP', [(u'different', u'JJ'), (u'kinds', u'NNS')]),\n",
       " ImmutableTree('NP', [(u'comfortable', u'JJ')]),\n",
       " ImmutableTree('NP', [(u'unclassified', u'JJ'), (u'message', u'NN')]),\n",
       " ImmutableTree('NP', [(u'Michael', u'NNP'), (u'``', u'``'), (u'Pee', u'NNP'), (u'Wee', u'NNP')])]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkscore.incorrect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Develop a chunker for one of the chunk types in the CoNLL corpus using a regular-expression based chunk grammar RegexpChunk. Use any combination of rules for chunking, chinking, merging or splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
