{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist # need this to access \"most_common\" method\n",
    "\n",
    "import urllib2\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from __future__  import division\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pay attention here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Please notice that when reading text from a file you need to first convert the str in files to unicode:\n",
    "\n",
    "https://stackoverflow.com/questions/491921/unicode-utf-8-reading-and-writing-to-files-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this exercise I want to read text from a document, then use Unigram, Bigram, or Trigram chunk parsers to parse the text messages. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Remember, to parse a sentence with chunk parser, you need to first tag the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "f = codecs.open('7_other_exercise_1.txt', 'r', \"utf-8\")\n",
    "raw = f.read()\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Sentiment',\n",
       " u'analysis',\n",
       " u'(',\n",
       " u'sometimes',\n",
       " u'known',\n",
       " u'as',\n",
       " u'opinion',\n",
       " u'mining',\n",
       " u'or',\n",
       " u'emotion']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Sentiment', 'NN'),\n",
       " (u'analysis', 'NN'),\n",
       " (u'(', '('),\n",
       " (u'sometimes', 'RB'),\n",
       " (u'known', 'VBN'),\n",
       " (u'as', 'IN'),\n",
       " (u'opinion', 'NN'),\n",
       " (u'mining', 'NN'),\n",
       " (u'or', 'CC'),\n",
       " (u'emotion', 'NN')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_tags = nltk.pos_tag(tokens)\n",
    "token_tags[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use BigramChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence): \n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('NP', [(u'Sentiment', 'NN'), (u'analysis', 'NN')]), (u'(', '('), (u'sometimes', 'RB'), (u'known', 'VBN'), (u'as', 'IN'), Tree('NP', [(u'opinion', 'NN'), (u'mining', 'NN')]), (u'or', 'CC'), Tree('NP', [(u'emotion', 'NN'), (u'AI', 'NNP')]), (u')', ')'), Tree('NP', [(u'refers', 'NNS')]), (u'to', 'TO'), Tree('NP', [(u'the', 'DT'), (u'use', 'NN')]), (u'of', 'IN'), Tree('NP', [(u'natural', 'JJ'), (u'language', 'NN'), (u'processing', 'NN')]), (u',', ','), Tree('NP', [(u'text', 'JJ'), (u'analysis', 'NN')]), (u',', ','), Tree('NP', [(u'computational', 'JJ'), (u'linguistics', 'NNS')]), (u',', ','), (u'and', 'CC'), Tree('NP', [(u'biometrics', 'NNS')]), (u'to', 'TO'), (u'systematically', 'RB'), (u'identify', 'VB'), (u',', ','), (u'extract', 'VB'), (u',', ','), (u'quantify', 'VB'), (u',', ','), (u'and', 'CC'), (u'study', 'VB'), Tree('NP', [(u'affective', 'JJ'), (u'states', 'NNS')]), (u'and', 'CC'), Tree('NP', [(u'subjective', 'JJ'), (u'information', 'NN')]), (u'.', '.'), Tree('NP', [(u'Sentiment', 'NN'), (u'analysis', 'NN')]), (u'is', 'VBZ'), (u'widely', 'RB'), (u'applied', 'VBN'), (u'to', 'TO'), Tree('NP', [(u'voice', 'NN')]), (u'of', 'IN'), Tree('NP', [(u'the', 'DT'), (u'customer', 'NN'), (u'materials', 'NNS'), (u'such', 'JJ')]), (u'as', 'IN'), Tree('NP', [(u'reviews', 'NNS')]), (u'and', 'CC'), Tree('NP', [(u'survey', 'NN'), (u'responses', 'NNS')]), (u',', ','), Tree('NP', [(u'online', 'NN')]), (u'and', 'CC')]\n"
     ]
    }
   ],
   "source": [
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print (bigram_chunker.parse(token_tags)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now I want to find all the patterns that look like \"verb to noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Sentiment', 'NN'), (u'analysis', 'NN'), (u'(', '('), (u'sometimes', 'RB'), (u'known', 'VBN'), (u'as', 'IN'), (u'opinion', 'NN'), (u'mining', 'NN'), (u'or', 'CC'), (u'emotion', 'NN'), (u'AI', 'NNP'), (u')', ')'), (u'refers', 'NNS'), (u'to', 'TO'), (u'the', 'DT'), (u'use', 'NN'), (u'of', 'IN'), (u'natural', 'JJ'), (u'language', 'NN'), (u'processing', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "pattern = '''\n",
    "V2N: {<V.*><TO><V.*>}\n",
    "\n",
    "'''\n",
    "chunk_parser = nltk.RegexpParser(pattern)\n",
    "print(chunk_parser.parse(token_tags)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
